{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def get_col_dtype(col):\n",
    "        \"\"\"\n",
    "        Identify datatype with pandas \n",
    "        \"\"\"\n",
    "        if col.dtype ==\"object\":\n",
    "            try:\n",
    "                col_new = pd.to_datetime(col)\n",
    "                return col_new.dtype\n",
    "            except:\n",
    "                try:\n",
    "                    col_new = pd.to_numeric(col)\n",
    "                    print(col_new.dtype)\n",
    "                    return col_new.dtype\n",
    "                except:\n",
    "                    return \"Object\"\n",
    "        if col.dtype ==\"bool\": return \"Object\"\n",
    "        else:\n",
    "            if col.dtype == 'float64' or col.dtype == 'int64':\n",
    "                return 'Numeric'\n",
    "            else:      \n",
    "                return col.dtype\n",
    "\n",
    "\n",
    "testdf = pd.read_csv('../../Benchmark-Labeled-Data/data_test.csv')\n",
    "test_metadata = pd.read_csv('../../Benchmark-Labeled-Data/Metadata/meta_data.csv')\n",
    "\n",
    "print(len(testdf),len(test_metadata))\n",
    "test_merged = pd.merge(testdf,test_metadata,on='Record_id')\n",
    "print(len(test_merged))\n",
    "\n",
    "print(test_merged)\n",
    "y_true = test_merged.y_act.values.tolist()\n",
    "print(y_true)\n",
    "print(len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pandas = []\n",
    "prv_csv_name,csv_name = '',''\n",
    "exception_indices = []\n",
    "\n",
    "for index, row in test_merged.iterrows():     \n",
    "    if index%100==0:  print(index)\n",
    "    col = row['Attribute_name']\n",
    "    prv_csv_name = csv_name\n",
    "    csv_name = '../../RawCSV/' + row['name']\n",
    "    \n",
    "    print(csv_name)\n",
    "    print(col)\n",
    "\n",
    "    if prv_csv_name != csv_name:  df = pd.read_csv(csv_name,encoding='latin1')\n",
    "    \n",
    "    try:\n",
    "        curtype = get_col_dtype(df[col])\n",
    "    except KeyError:\n",
    "        curtype = 'Object'\n",
    "    print(curtype)\n",
    "    y_pandas.append(curtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label = {\n",
    "    'Numeric': 0,\n",
    "    \"datetime64[ns]\": 2,\n",
    "    'datetime64[ns, UTC]': 2,\n",
    "    'datetime64[ns, pytz.FixedOffset(-240)]': 2,\n",
    "    'datetime64[ns, pytz.FixedOffset(-300)]': 2,\n",
    "    'datetime64[ns, pytz.FixedOffset(-60)]': 2,\n",
    "    'Object':8,\n",
    "    'bool':8\n",
    "}\n",
    "\n",
    "dict_label_true = {\n",
    "    'numeric': 0,\n",
    "    'categorical': 1,\n",
    "    'datetime': 2,\n",
    "    'sentence': 3,\n",
    "    'url': 4,\n",
    "    'embedded-number': 5,\n",
    "    'list': 6,\n",
    "    'not-generalizable': 7,\n",
    "    'context-specific': 8\n",
    "}\n",
    "\n",
    "y_true = [dict_label_true[str(i)] for i in y_true]\n",
    "y_pandas = [dict_label[str(i)] for i in y_pandas]\n",
    "# print(y_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def PrintMetrics(y_true, y_pred):\n",
    "    print(f'Accuracy: {accuracy_score(y_true, y_pred)}')\n",
    "\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(matrix)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 score: {f1}')\n",
    "\n",
    "\n",
    "PrintMetrics(y_true, y_pandas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
